---
output:
  word_document: default
  html_document: default
---


```{r}
original_dataset_dir <- "~/Documents/R/train 2"
base_dir <- "~/Documents/R/cats.vs.dogs"
dir.create(base_dir)
train_dir <- file.path(base_dir, "train")
dir.create(train_dir)
validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)
test_dir <- file.path(base_dir, "test")
dir.create(test_dir)

train_cats_dir <- file.path(train_dir, "cats")
dir.create(train_cats_dir)

train_dogs_dir <- file.path(train_dir, "dogs")
dir.create(train_dogs_dir)

validation_cats_dir <- file.path(validation_dir, "cats")
dir.create(validation_cats_dir)

validation_dogs_dir <- file.path(validation_dir, "dogs")
dir.create(validation_dogs_dir)

test_cats_dir <- file.path(test_dir, "cats")
dir.create(test_cats_dir)

test_dogs_dir <- file.path(test_dir, "dogs")
dir.create(test_dogs_dir)

fnames <- paste0("cat.", 1:1000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir)) 

fnames <- paste0("cat.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir))

fnames <- paste0("cat.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir))

fnames <- paste0("dog.", 1:1000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))

fnames <- paste0("dog.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir)) 

fnames <- paste0("dog.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))
```
Split the data into training, test and validation sets and verified the number of images in each.
```{r}
cat("total training cat images:", length(list.files(train_cats_dir)), "\n")
```

```{r}
cat("total training dog images:", length(list.files(train_dogs_dir)), "\n")
```

```{r}
cat("total validation cat images:", length(list.files(validation_cats_dir)), "\n")
```

```{r}
cat("total validation dog images:", length(list.files(validation_dogs_dir)), "\n")
```

```{r}
cat("total test cat images:", length(list.files(test_cats_dir)), "\n")
```

```{r}
 cat("total test dog images:", length(list.files(test_dogs_dir)), "\n")
```

```{r}
library(keras)

model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
summary(model)
```

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```

```{r}
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
```

```{r}
batch <- generator_next(train_generator)
str(batch)
```


```{r}
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator,
  validation_steps = 50
)
```
Built and visualized the original model
```{r}
model %>% save_model_hdf5("cats_and_dogs_small_1.h5")
```

```{r}
plot(history)
```
Created a second model using data augmentation.
```{r}
model2 <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")  
  
model2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```

```{r}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)

test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 32,
  class_mode = "binary"
)

validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 32,
  class_mode = "binary"
)
```

```{r}
history <- model2 %>% fit(
  train_generator,
  epochs = 30,
  validation_data = validation_generator,
)
```


```{r}
base_dir2 <- "~/Documents/R/cats.vs.dogs2"
dir.create(base_dir2)
train_dir2 <- file.path(base_dir2, "train")
dir.create(train_dir2)
validation_dir2 <- file.path(base_dir2, "validation")
dir.create(validation_dir2)
test_dir2 <- file.path(base_dir2, "test")
dir.create(test_dir2)

train_cats_dir2 <- file.path(train_dir2, "cats")
dir.create(train_cats_dir2)

train_dogs_dir2 <- file.path(train_dir2, "dogs")
dir.create(train_dogs_dir2)

validation_cats_dir2 <- file.path(validation_dir2, "cats")
dir.create(validation_cats_dir2)

validation_dogs_dir2 <- file.path(validation_dir2, "dogs")
dir.create(validation_dogs_dir2)

test_cats_dir2 <- file.path(test_dir2, "cats")
dir.create(test_cats_dir2)

test_dogs_dir2 <- file.path(test_dir2, "dogs")
dir.create(test_dogs_dir2)

fnames <- paste0("cat.", 1:1300, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir2)) 

fnames <- paste0("cat.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir2))

fnames <- paste0("cat.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir2))

fnames <- paste0("dog.", 1:1300, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir2))

fnames <- paste0("dog.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir2)) 

fnames <- paste0("dog.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir2))
```
Increased the training data to 1300 images for a third model. 
```{r}
cat("total training cat images:", length(list.files(train_cats_dir2)), "\n")
```

```{r}
cat("total training dog images:", length(list.files(train_dogs_dir2)), "\n")
```

```{r}
cat("total validation cat images:", length(list.files(validation_cats_dir2)), "\n")
```

```{r}
cat("total validation dog images:", length(list.files(validation_dogs_dir2)), "\n")
```

```{r}
cat("total test cat images:", length(list.files(test_cats_dir2)), "\n")
```

```{r}
 cat("total test dog images:", length(list.files(test_dogs_dir2)), "\n")
```

```{r}
model3 <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model3 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```

```{r}
train_datagen2 <- image_data_generator(rescale = 1/255)
validation_datagen2 <- image_data_generator(rescale = 1/255)

train_generator2 <- flow_images_from_directory(
  train_dir2,
  train_datagen2,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

validation_generator2 <- flow_images_from_directory(
  validation_dir2,
  validation_datagen2,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
```

```{r}
history3 <- model3 %>% fit(
  train_generator2,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator2,
  validation_steps = 50
)
```




```{r}
base_dir3 <- "~/Documents/R/cats.vs.dogs3"
dir.create(base_dir3)
train_dir3 <- file.path(base_dir3, "train")
dir.create(train_dir3)
validation_dir3 <- file.path(base_dir3, "validation")
dir.create(validation_dir3)
test_dir3 <- file.path(base_dir3, "test")
dir.create(test_dir3)

train_cats_dir3 <- file.path(train_dir3, "cats")
dir.create(train_cats_dir3)

train_dogs_dir3 <- file.path(train_dir3, "dogs")
dir.create(train_dogs_dir3)

validation_cats_dir3 <- file.path(validation_dir3, "cats")
dir.create(validation_cats_dir3)

validation_dogs_dir3 <- file.path(validation_dir3, "dogs")
dir.create(validation_dogs_dir3)

test_cats_dir3 <- file.path(test_dir3, "cats")
dir.create(test_cats_dir3)

test_dogs_dir3 <- file.path(test_dir3, "dogs")
dir.create(test_dogs_dir3)

fnames <- paste0("cat.", 1:1800, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir3)) 

fnames <- paste0("cat.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir3))

fnames <- paste0("cat.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir3))

fnames <- paste0("dog.", 1:1800, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir3))

fnames <- paste0("dog.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir3)) 

fnames <- paste0("dog.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir3))
```
Increased the training data to 1800 images for a fourth version of the model. 
```{r}
cat("total training cat images:", length(list.files(train_cats_dir3)), "\n")
```

```{r}
cat("total training dog images:", length(list.files(train_dogs_dir3)), "\n")
```

```{r}
cat("total validation cat images:", length(list.files(validation_cats_dir3)), "\n")
```

```{r}
cat("total validation dog images:", length(list.files(validation_dogs_dir3)), "\n")
```

```{r}
cat("total test cat images:", length(list.files(test_cats_dir3)), "\n")
```

```{r}
 cat("total test dog images:", length(list.files(test_dogs_dir3)), "\n")
```

```{r}
model4 <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model4 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("acc")
)
```

```{r}
train_datagen3 <- image_data_generator(rescale = 1/255)
validation_datagen3 <- image_data_generator(rescale = 1/255)

train_generator3 <- flow_images_from_directory(
  train_dir3,
  train_datagen3,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)

validation_generator3 <- flow_images_from_directory(
  validation_dir3,
  validation_datagen3,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
```

```{r}
history4 <- model4 %>% fit(
  train_generator3,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator3,
  validation_steps = 50
)
```
This model with 1800 training images had the best performance overall.


Using a Pretrained model:
```{r}
library(keras)

conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(150, 150, 3)
)
```

```{r}
summary(conv_base)
```

```{r}
base_dir4 <- "~/Documents/R/cats.vs.dogs4"
train_dir4 <- file.path(base_dir, "train")
validation_dir4 <- file.path(base_dir, "validation")
test_dir4 <- file.path(base_dir, "test")

datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count) {
  
  features <- array(0, dim = c(sample_count, 4, 4, 512))  
  labels <- array(0, dim = c(sample_count))
  
  generator <- flow_images_from_directory(
    directory = directory,
    generator = datagen,
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "binary"
  )
  
  i <- 0
  while(TRUE) {
    batch <- generator_next(generator)
    inputs_batch <- batch[[1]]
    labels_batch <- batch[[2]]
    features_batch <- conv_base %>% predict(inputs_batch)
    
    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
    features[index_range,,,] <- features_batch
    labels[index_range] <- labels_batch
    
    i <- i + 1
    if (i * batch_size >= sample_count)
      break
  }
  
  list(
    features = features, 
    labels = labels
  )
}

train <- extract_features(train_dir4, 2000)
validation <- extract_features(validation_dir4, 1000)
test <- extract_features(test_dir4, 1000)
```

```{r}
reshape_features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)
```

```{r}
model5 <- keras_model_sequential() %>% 
  layer_dense(units = 256, activation = "relu", 
              input_shape = 4 * 4 * 512) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model5 %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history5 <- model5 %>% fit(
  train$features, train$labels,
  epochs = 30,
  batch_size = 20,
  validation_data = list(validation$features, validation$labels)
)
```
The pretrained model performed well, but is still showing signs of overfitting.

Used the pretrained model with 1300 training images:
```{r}
base_dir6 <- "~/Documents/R/cats.vs.dogs6"

datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count) {
  
  features <- array(0, dim = c(sample_count, 4, 4, 512))  
  labels <- array(0, dim = c(sample_count))
  
  generator <- flow_images_from_directory(
    directory = directory,
    generator = datagen,
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "binary"
  )
  
  i <- 0
  while(TRUE) {
    batch <- generator_next(generator)
    inputs_batch <- batch[[1]]
    labels_batch <- batch[[2]]
    features_batch <- conv_base %>% predict(inputs_batch)
    
    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
    features[index_range,,,] <- features_batch
    labels[index_range] <- labels_batch
    
    i <- i + 1
    if (i * batch_size >= sample_count)
      break
  }
  
  list(
    features = features, 
    labels = labels
  )
}

train <- extract_features(train_dir2, 2600)
validation <- extract_features(validation_dir2, 1000)
test <- extract_features(test_dir2, 1000)
```

```{r}
reshape_features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)
```

```{r}
model7 <- keras_model_sequential() %>% 
  layer_dense(units = 256, activation = "relu", 
              input_shape = 4 * 4 * 512) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model7 %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history7 <- model7 %>% fit(
  train$features, train$labels,
  epochs = 30,
  batch_size = 20,
  validation_data = list(validation$features, validation$labels)
)
```
```


Used the pretrained model with 1800 images:
```{r}
base_dir5 <- "~/Documents/R/cats.vs.dogs5"

datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count) {
  
  features <- array(0, dim = c(sample_count, 4, 4, 512))  
  labels <- array(0, dim = c(sample_count))
  
  generator <- flow_images_from_directory(
    directory = directory,
    generator = datagen,
    target_size = c(150, 150),
    batch_size = batch_size,
    class_mode = "binary"
  )
  
  i <- 0
  while(TRUE) {
    batch <- generator_next(generator)
    inputs_batch <- batch[[1]]
    labels_batch <- batch[[2]]
    features_batch <- conv_base %>% predict(inputs_batch)
    
    index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
    features[index_range,,,] <- features_batch
    labels[index_range] <- labels_batch
    
    i <- i + 1
    if (i * batch_size >= sample_count)
      break
  }
  
  list(
    features = features, 
    labels = labels
  )
}

train <- extract_features(train_dir3, 3600)
validation <- extract_features(validation_dir3, 1000)
test <- extract_features(test_dir3, 1000)
```

```{r}
reshape_features <- function(features) {
  array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
validation$features <- reshape_features(validation$features)
test$features <- reshape_features(test$features)
```

```{r}
model6 <- keras_model_sequential() %>% 
  layer_dense(units = 256, activation = "relu", 
              input_shape = 4 * 4 * 512) %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = "sigmoid")

model6 %>% compile(
  optimizer = optimizer_rmsprop(lr = 2e-5),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history6 <- model6 %>% fit(
  train$features, train$labels,
  epochs = 30,
  batch_size = 20,
  validation_data = list(validation$features, validation$labels)
)
```
This model resulted in the best performance overall.

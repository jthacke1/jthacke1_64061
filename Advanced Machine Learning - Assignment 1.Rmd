---
title: "Advanced Machine Learning - Assignment 1"
author: "Julia Thacker"
date: "2/13/2022"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(keras)

imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```
Imported the IMDB dataset and limited to the top 10,000 most common words.
```{r}
vectorize_sequences <- function(sequences, dimension = 10000) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    results[i, sequences[[i]]] <- 1
  results
}

x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
```
Vectorized the data.
```{r}
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```
Vectorized the labels.
```{r}
val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```
Created a validation set consisting of 10,000 samples from the training data.
```{r}
originalmodel <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```
Replicated the original model from the class example for comparison purposes.
```{r}
originalmodel %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
history <- originalmodel %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r}
plot(history)
```
Visualized the original model.

My first test: 3 layers instead of 2
```{r}
model2 <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model2 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
history2 <- model2 %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r}
str(history2)
```

```{r}
plot(history2)
```

```{r}
library(ggplot2)
library(tidyr)
plot_training_losses <- function(losses) {
  loss_names <- names(losses)
  losses <- as.data.frame(losses)
  losses$epoch <- seq_len(nrow(losses))
  losses %>% 
    gather(model, loss, loss_names[[1]], loss_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = loss, colour = model)) +
    geom_point()
}
```
Plotted the results compared to the original model.
```{r}
plot_training_losses(losses = list(
  original_model = history$metrics$val_loss,
 model2 = history2$metrics$val_loss
))
```
Increasing the number of layers did not improve the model.

```{r}
model3 <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```
Created a model with 64 units instead of the original 16.
```{r}
model3 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
history3 <- model3 %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r}
str(history3)
plot(history3)
```

```{r}
plot_training_losses(losses = list(
  original_model = history$metrics$val_loss,
 model3 = history3$metrics$val_loss
))
```
Visualized the model.
Increasing the number of hidden units also had a negative impact on the performance of the model.

```{r}
model3b <- keras_model_sequential() %>% 
  layer_dense(units = 2, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 2, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```
Adjusted the number of hidden units again, decreasing to only 2, to see the results.
```{r}
model3b %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
history3b <- model3b %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
```

```{r}
str(history3b)
plot(history3b)
```

```{r}
plot_training_losses(losses = list(
  original_model = history$metrics$val_loss,
 model3b = history3b$metrics$val_loss
))
```
The smaller number of hidden units reduced the validation loss.

```{r}
model4 <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
model4 %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)
```
Performed the mse loss function instead of binary_crossentropy.
```{r}
history4 <- model4 %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
str(history4)
plot(history4)
```

```{r}
plot_training_losses(losses = list(
  original_model = history$metrics$val_loss,
 model4 = history4$metrics$val_loss
))
```
Using the mse loss function has produced the best results so far. This model has less validation loss and is not experiencing as severe overfitting as the original model.


```{r}
model5 <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "tanh", input_shape = c(10000)) %>% 
  layer_dense(units = 16, activation = "tanh") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```
Created a model using the tanh activation method instead of relu.
```{r}
model5 %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
history5 <- model5 %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
str(history5)
plot(history5)
```

```{r}
plot_training_losses(losses = list(
  original_model = history$metrics$val_loss,
  model5 = history5$metrics$val_loss
))
```
Using the tahn activation method still results in overfitting and additional validation loss compared to the original model.


In an attempt to get a model that could perform better on validation, I decided to use the regularization technique. I tried a few variations of this method before concluding that the code below produced the best results. My final decision was to use a weight of 0.01. I also decided to use the mse loss function instead of binary_crossentropy, because this method appeared to produce the best results during my prior trials.
```{r}
regularization_model <- keras_model_sequential() %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.01),
              activation = "relu", input_shape = c(10000)) %>% 
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.01),
              activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

regularization_model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("acc")
)
```

```{r}
regularization_model_hist <- regularization_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_test, y_test)
)
```

```{r}
plot_training_losses(losses = list(
  original_model = history$metrics$val_loss,
  regularization_model = regularization_model_hist$metrics$val_loss
))
```
This final regularization model performed significantly better than the original model. Both the training and validation data in this model were resistant to overfitting and primarily showed a reduction in loss with every epoch. Although the accuracy of the validation data does dip at some points, the accuracy is not as low as some of the previous model attempts, and the accuracy of the validation data is much more similar to the accuracy of the training data than any of the previous iterations.
